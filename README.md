# Bachelor-s-Thesis
Text to Image Synthesis Using Text Conditioned Auxiliary Classifier  and Generative Adversarial Network




# ABSTRACT:  
The project aims to generate realistic images from textual descriptions using Text Conditioned Auxiliary Classifier Generative Adversarial Network. TacGAN is a generative adversarial network (GAN) that uses a text encoder to encode the textual description and a conditional generator to generate the corresponding image. The generator is trained to minimize the difference between the generated image and the real image, while the discriminator is trained to distinguish between real and generated images.

Generative Adversarial Networks (GANs) have found some very interesting implementations in the past year like a deep fake that can animate your face with just your voice, a neural GAN to fight fake news, a CycleGAN to visualize the effects of climate change, and more. Text-to-image synthesis using TAC-GAN is also a challenging task that involves developing an effective way to encode text into a visual feature space, using attention mechanisms to focus on relevant parts of the text, and training the GAN to generate high-quality images that are faithful to the input descriptions.
 
The developed methods have shown prominent progress in the visual quality of the synthesized images, but they still face challenges in the image synthesis of details. The objective of doing a project on text-to-image synthesis using TAC-GAN is to generate high-quality images from textual descriptions for various applications such as photo editing or computer-aided content creation. Solving these challenges requires creativity, critical thinking, and problem-solving skills, making it a challenging and rewarding project to work on. In this project, there is a number of applications of distinctive published papers of many researchers encircling GAN architecture.
